{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "       \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    " \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (1.16.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: boto3 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (1.15.16)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (2020.9.27)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.16 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from boto3->transformers) (1.18.16)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\asus\\anaconda3\\envs\\env\\lib\\site-packages (from botocore<1.19.0,>=1.18.16->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train=pd.read_csv(\"../dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.RandomWordAug(action=\"swap\")\n",
    "df = pd.DataFrame(columns=['text','intent'])\n",
    "for i,row in train.iterrows():\n",
    "  augmented_text = aug.augment(row['text'])\n",
    "  df = df.append({'text': augmented_text,'intent': row['intent']}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(aug_src=\"wordnet\")\n",
    "df1 = pd.DataFrame(columns=['text','intent'])\n",
    "for i,row in train.iterrows():\n",
    "  augmented_text = aug.augment(row['text'])\n",
    "  df1 = df1.append({'text': augmented_text,'intent': row['intent']}, ignore_index=True)\n",
    "train = train.append(df).reset_index(drop=True)\n",
    "train = train.append(df1).reset_index(drop=True)\n",
    "\n",
    "train = train.append(train).reset_index(drop=True)\n",
    "train = train.append(train).reset_index(drop=True)\n",
    "train.shape\n",
    "\n",
    "sentences = train.text.values\n",
    "labels = train.intent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      " Original:   i want to take a  from delhi at 838 am and arrive in mumbai at 1110 in the morning\n",
      "Tokenized:  ['i', 'want', 'to', 'take', 'a', 'from', 'delhi', 'at', '83', '##8', 'am', 'and', 'arrive', 'in', 'mumbai', 'at', '111', '##0', 'in', 'the', 'morning']\n",
      "Token IDs:  [1045, 2215, 2000, 2202, 1037, 2013, 6768, 2012, 6640, 2620, 2572, 1998, 7180, 1999, 8955, 2012, 11118, 2692, 1999, 1996, 2851]\n",
      "Max sentence length:  25\n",
      "TrainAvailable\n",
      "TrainFare\n",
      "GetDistance\n",
      "TrainRoute\n",
      "Original:  TrainAvailable\n",
      "Token IDs: 0\n",
      "Original:  TrainAvailable\n",
      "Token IDs: 0\n",
      "Original:  TrainAvailable\n",
      "Token IDs: 0\n",
      "Original:  TrainAvailable\n",
      "Token IDs: 0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "max_len = 0\n",
    "\n",
    " \n",
    "for sent in sentences:\n",
    " \n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    " \n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "input_labels = []\n",
    " \n",
    "k = 1;\n",
    "mp = {}\n",
    "mpp= {}\n",
    "for sent in labels:\n",
    "    mp[sent]=0;\n",
    "for sent in labels:\n",
    "    if(mp[sent] == 0):\n",
    "        print(sent)\n",
    "        mp[sent] = k\n",
    "        mpp[k]=sent\n",
    "        k = k + 1\n",
    "    input_labels.append(mp[sent]-1)\n",
    "print('Original: ', labels[0])\n",
    "print('Token IDs:', input_labels[0])\n",
    "print('Original: ', labels[1])\n",
    "print('Token IDs:', input_labels[1])\n",
    "print('Original: ', labels[2])\n",
    "print('Token IDs:', input_labels[2])\n",
    "print('Original: ', labels[3])\n",
    "print('Token IDs:', input_labels[3])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   i want to take a  from delhi at 838 am and arrive in mumbai at 1110 in the morning\n",
      "Token IDs: tensor([  101,  1045,  2215,  2000,  2202,  1037,  2013,  6768,  2012,  6640,\n",
      "         2620,  2572,  1998,  7180,  1999,  8955,  2012, 11118,  2692,  1999,\n",
      "         1996,  2851,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "  302 training samples\n",
      "   34 validation samples\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (4, 768)\n",
      "classifier.bias                                                 (4,)\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    " \n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True,  \n",
    "                        max_length = 64,            \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,    \n",
    "                        return_tensors = 'pt',      \n",
    "                        truncation = True,\n",
    "                   )\n",
    "    \n",
    "   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(input_labels)\n",
    "\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "\"\"\"# Split dataset in Train and Validation\"\"\"\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,   \n",
    "            sampler = RandomSampler(train_dataset),  \n",
    "            batch_size = batch_size  \n",
    "        )\n",
    " \n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,  \n",
    "            sampler = SequentialSampler(val_dataset),  \n",
    "            batch_size = batch_size \n",
    "        )\n",
    " \n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",  \n",
    "    num_labels = k-1,    \n",
    "    output_attentions = False,  \n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,  \n",
    "                  eps = 1e-8  \n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    " \n",
    "epochs = 4\n",
    " \n",
    "total_steps = len(train_dataloader) * epochs\n",
    " \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,  \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    ''' \n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "     \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 1.16\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation Loss: 0.81\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.74\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.34\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.27\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:54 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Training Model\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\")  \n",
    "seed_val = 42\n",
    " \n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    " \n",
    "training_stats = []\n",
    "  \n",
    "total_t0 = time.time()\n",
    "  \n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "   \n",
    " \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "  \n",
    "    t0 = time.time()\n",
    "  \n",
    "    total_train_loss = 0\n",
    " \n",
    "    model.train()\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "  \n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    " \n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    " \n",
    "        model.zero_grad()        \n",
    " \n",
    "       \n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    " \n",
    "     \n",
    "        total_train_loss += loss.item()\n",
    " \n",
    "        loss.backward()\n",
    " \n",
    "       \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    " \n",
    "        optimizer.step()\n",
    " \n",
    "        scheduler.step()\n",
    " \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "   \n",
    " \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    " \n",
    "    t0 = time.time()\n",
    " \n",
    "    model.eval()\n",
    " \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    " \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "     \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():        \n",
    " \n",
    "          \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "         \n",
    "        total_eval_loss += loss.item()\n",
    " \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    " \n",
    "       \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    " \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    " \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "  \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    " \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    " \n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show me some train from mumbai to delhi', 'route of train from delhi to mumbai']\n",
      "Predicting labels for 2 test sentences...\n",
      "    DONE.\n",
      "TrainAvailable\n",
      "TrainRoute\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"# Prediction\"\"\"\n",
    "\n",
    "sentences = [\n",
    "  \"show me some train from mumbai to delhi\",\n",
    "  \"route of train from delhi to mumbai\",\n",
    "]\n",
    "print(sentences)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "   \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  \n",
    "                        return_tensors = 'pt', \n",
    "                        truncation=True,    \n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    " \n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks )\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    " \n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  b_input_ids, b_input_mask  = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      \n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "  predictions.append(logits)\n",
    "\n",
    "\n",
    "print('    DONE.')\n",
    "\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    " \n",
    "print(mpp[flat_predictions[0]+1])\n",
    "print(mpp[flat_predictions[1]+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../output/intent/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = '../output/intent/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    " \n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    " \n",
    "model.to(device)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
